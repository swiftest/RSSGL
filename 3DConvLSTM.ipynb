{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplecv.util import config\n",
    "from simplecv.core.config import AttrDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'RSSGL.RSSGL_Pavia'\n",
    "\n",
    "cfg = config.import_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AttrDict.from_dict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.save_ckpt_interval_epoch', '9999']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts = ['train.save_ckpt_interval_epoch', '9999']\n",
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.update_from_list(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'type': 'RSSGL',\n",
       "  'params': {'in_channels': 103,\n",
       "   'num_classes': 9,\n",
       "   'block_channels': (96, 128, 192, 256),\n",
       "   'inner_dim': 128,\n",
       "   'reduction_ratio': 1.0}},\n",
       " 'data': {'train': {'type': 'NewPaviaLoader',\n",
       "   'params': {'training': True,\n",
       "    'num_workers': 4,\n",
       "    'image_mat_path': './pavia/PaviaU.mat',\n",
       "    'gt_mat_path': './pavia/PaviaU_gt.mat',\n",
       "    'sample_percent': 0.01,\n",
       "    'batch_size': 10}},\n",
       "  'test': {'type': 'NewPaviaLoader',\n",
       "   'params': {'training': False,\n",
       "    'num_workers': 4,\n",
       "    'image_mat_path': './pavia/PaviaU.mat',\n",
       "    'gt_mat_path': './pavia/PaviaU_gt.mat',\n",
       "    'sample_percent': 0.01,\n",
       "    'batch_size': 10}}},\n",
       " 'optimizer': {'type': 'sgd',\n",
       "  'params': {'momentum': 0.9, 'weight_decay': 0.001}},\n",
       " 'learning_rate': {'type': 'poly',\n",
       "  'params': {'base_lr': 0.005, 'power': 0.8, 'max_iters': 1000}},\n",
       " 'train': {'forward_times': 1,\n",
       "  'num_iters': 2,\n",
       "  'eval_per_epoch': True,\n",
       "  'summary_grads': False,\n",
       "  'summary_weights': False,\n",
       "  'eval_after_train': True,\n",
       "  'resume_from_last': False,\n",
       "  'early_stopping': True,\n",
       "  'early_epoch': 0,\n",
       "  'early_num': 10,\n",
       "  'PATH': './Optimal_net.pt',\n",
       "  'test_oa': [0.0],\n",
       "  'save_ckpt_interval_epoch': 9999},\n",
       " 'test': {'draw': {'image_size': (610, 340),\n",
       "   'palette': [[0, 0, 0],\n",
       "    [192, 192, 192],\n",
       "    [0, 255, 1],\n",
       "    [0, 255, 255],\n",
       "    [0, 128, 1],\n",
       "    [255, 0, 254],\n",
       "    [165, 82, 40],\n",
       "    [129, 0, 127],\n",
       "    [255, 0, 0],\n",
       "    [255, 255, 0]]}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from module import RSSGL\n",
    "from simplecv.module.model_builder import make_model\n",
    "from simplecv import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resnet18': <function simplecv.module._resnets.resnet18(pretrained=False, progress=True, **kwargs)>,\n",
       " 'resnet34': <function simplecv.module._resnets.resnet34(pretrained=False, progress=True, **kwargs)>,\n",
       " 'resnet50': <function simplecv.module._resnets.resnet50(pretrained=False, progress=True, **kwargs)>,\n",
       " 'resnet101': <function simplecv.module._resnets.resnet101(pretrained=False, progress=True, **kwargs)>,\n",
       " 'resnext50_32x4d': <function simplecv.module._resnets.resnext50_32x4d(pretrained=False, progress=True, **kwargs)>,\n",
       " 'resnext101_32x4d': <function simplecv.module._resnets.resnext101_32x4d(pretrained=False, progress=True, **kwargs)>,\n",
       " 'resnext101_32x8d': <function simplecv.module._resnets.resnext101_32x8d(pretrained=False, progress=True, **kwargs)>,\n",
       " 'resnet_encoder': simplecv.module.resnet.ResNetEncoder,\n",
       " 'RSSGL': module.RSSGL.RSSGL}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RSSGL'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = cfg['model']\n",
    "model_type = config['type']\n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = registry.MODEL[model_type](config['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RSSGL(\n",
       "  (feature_ops): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(103, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): GroupNorm(4, 96, eps=1e-05, affine=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Si_ConvLSTM(\n",
       "        (cell0): ConvLSTMCell(\n",
       "          (Wxi): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxf): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxc): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxo): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Who): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "        )\n",
       "        (cell1): ConvLSTMCell(\n",
       "          (Wxi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxo): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Who): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (ca): ChannelAttention(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "          (fc1): Conv2d(48, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(3, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (sa): SpatialAttention(\n",
       "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): GroupNorm(4, 96, eps=1e-05, affine=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Identity()\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Si_ConvLSTM(\n",
       "        (cell0): ConvLSTMCell(\n",
       "          (Wxi): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxf): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxc): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxo): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Who): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "        )\n",
       "        (cell1): ConvLSTMCell(\n",
       "          (Wxi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxo): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Who): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (ca): ChannelAttention(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (sa): SpatialAttention(\n",
       "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Identity()\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(128, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Si_ConvLSTM(\n",
       "        (cell0): ConvLSTMCell(\n",
       "          (Wxi): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxf): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxc): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxo): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Who): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "        )\n",
       "        (cell1): ConvLSTMCell(\n",
       "          (Wxi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxo): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Who): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (ca): ChannelAttention(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (sa): SpatialAttention(\n",
       "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): GroupNorm(4, 192, eps=1e-05, affine=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Identity()\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Si_ConvLSTM(\n",
       "        (cell0): ConvLSTMCell(\n",
       "          (Wxi): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxf): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxc): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxo): Conv3d(1, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Who): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "        )\n",
       "        (cell1): ConvLSTMCell(\n",
       "          (Wxi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whi): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whf): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Whc): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "          (Wxo): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
       "          (Who): Conv3d(4, 4, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (ca): ChannelAttention(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (sa): SpatialAttention(\n",
       "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Identity()\n",
       "  )\n",
       "  (BasicBlock_list): ModuleList(\n",
       "    (0): BasicBlock(\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (spation_list): ModuleList(\n",
       "    (0): SpatialAttention(\n",
       "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (1): SpatialAttention(\n",
       "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (2): SpatialAttention(\n",
       "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (3): SpatialAttention(\n",
       "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (reduce_1x1convs): ModuleList(\n",
       "    (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (fuse_3x3convs): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Conv2d(128, 103, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (cls_pred_conv): Conv2d(103, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.device('cuda'))\n",
    "model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplecv.data.data_loader import make_dataloader\n",
    "import data.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'NewPaviaLoader',\n",
       " 'params': {'training': True,\n",
       "  'num_workers': 4,\n",
       "  'image_mat_path': './pavia/PaviaU.mat',\n",
       "  'gt_mat_path': './pavia/PaviaU_gt.mat',\n",
       "  'sample_percent': 0.01,\n",
       "  'batch_size': 10}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = cfg['data']['train']\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NewPaviaLoader'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_type = config['type']\n",
    "dataloader_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NewPaviaLoader': data.dataloader.NewPaviaLoader}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " 'num_workers': 4,\n",
       " 'image_mat_path': './pavia/PaviaU.mat',\n",
       " 'gt_mat_path': './pavia/PaviaU_gt.mat',\n",
       " 'sample_percent': 0.01,\n",
       " 'batch_size': 10}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_loader = registry.DATALOADER[dataloader_type](config['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_loader = make_dataloader(cfg['data']['test']) if 'test' in cfg['data'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindata_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n",
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n"
     ]
    }
   ],
   "source": [
    "for idx, (im, mask, w) in enumerate(traindata_loader):\n",
    "    print(im.shape, mask.shape, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 103, 624, 352]) torch.Size([1, 624, 352]) torch.Size([1, 624, 352])\n"
     ]
    }
   ],
   "source": [
    "for idx, (im, mask, w) in enumerate(testdata_loader):\n",
    "    print(im.shape, mask.shape, w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplecv.opt.learning_rate import make_learningrate\n",
    "import numpy as np\n",
    "from simplecv.util import registry\n",
    "from simplecv.interface import LearningRateBase\n",
    "import math\n",
    "from simplecv.opt.optimizer import make_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'poly', 'params': {'base_lr': 0.005, 'power': 0.8, 'max_iters': 1000}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = cfg['learning_rate']\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poly'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_type = config['type']\n",
    "lr_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multistep': simplecv.opt.learning_rate.MultiStepLearningRate,\n",
       " 'poly': simplecv.opt.learning_rate.PolyLearningRate,\n",
       " 'cosine': simplecv.opt.learning_rate.CosineAnnealingLearningRate}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_module = registry.LR[lr_type]\n",
    "lr_schedule = lr_module(**config['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_base_lr': 0.005, 'power': 0.8, 'max_iters': 1000}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_schedule.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_schedule.base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['optimizer']['params']['lr'] = lr_schedule.base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'momentum': 0.9, 'weight_decay': 0.001, 'lr': 0.005}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['optimizer']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cfg['optimizer']\n",
    "params = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sgd': torch.optim.sgd.SGD,\n",
       " 'adam': torch.optim.adam.Adam,\n",
       " 'fused_adam': apex.optimizers.fused_adam.FusedAdam}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sgd'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_type = config['type']\n",
    "opt_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = registry.OPT[opt_type](params=params, **config['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.simplecv_config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.005\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0.001\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplecv.core import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = trainer.Launcher(model_dir='./log/pavia/SSDGL/1.0_poly', \n",
    "                      model=model, \n",
    "                      optimizer=optimizer, \n",
    "                      lr_schedule=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GCLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from simplecv.interface import CVModule\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3_gn_relu(in_channel, out_channel, num_group):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channel, out_channel, 3, 1, 1), \n",
    "        nn.GroupNorm(num_group, out_channel), \n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "def gn_relu(in_channel, num_group):\n",
    "    return nn.Sequential(\n",
    "        nn.GroupNorm(num_group, in_channel), \n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "def downsample2x(in_channel, out_channel):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channel, out_channel, 3, 2, 1), \n",
    "        nn.ReLU(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"SSDGL.SSDGL_1_0_pavia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        assert hidden_channels % 2 == 0\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_features = 4\n",
    "\n",
    "        # self.padding = int((kernel_size - 1) / 2)\n",
    "        self.padding = tuple((int((i-1)/2) for i in kernel_size))\n",
    "\n",
    "        self.Wxi = nn.Conv3d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whi = nn.Conv3d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv3d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whf = nn.Conv3d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv3d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whc = nn.Conv3d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv3d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Who = nn.Conv3d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden, shape):\n",
    "        if self.Wci is None:\n",
    "            self.Wci = Variable(torch.zeros(1, hidden, shape[0], shape[1], shape[2])).cuda()\n",
    "            self.Wcf = Variable(torch.zeros(1, hidden, shape[0], shape[1], shape[2])).cuda()\n",
    "            self.Wco = Variable(torch.zeros(1, hidden, shape[0], shape[1], shape[2])).cuda()\n",
    "        else:\n",
    "            assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!'\n",
    "            assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!'\n",
    "            assert shape[2] == self.Wci.size()[4], 'Input Dimension Mismatched!'\n",
    "        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1], shape[2])).cuda(),\n",
    "                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1], shape[2])).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, step=8, effective_step=7):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.step = step\n",
    "        self.effective_step = effective_step\n",
    "        self._all_layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            name = 'cell{}'.format(i)\n",
    "            name_reverse = 'cell_reverse{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "            cell_reverse = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "            setattr(self, name, cell)\n",
    "            setattr(self, name_reverse, cell_reverse)\n",
    "            self._all_layers.append(cell)\n",
    "            self._all_layers.append(cell_reverse)\n",
    "        # self.recover_conv = nn.Conv2d(self.hidden_channels[0] * self.hidden_channels[1], \n",
    "        #                               self.hidden_channels[-1] * 8, 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        internal_state = []\n",
    "        internal_state_reverse = []\n",
    "        outputs = []\n",
    "        outputs_reverse = []\n",
    "        a = input.squeeze()\n",
    "        b = int(len(a) / 8)\n",
    "        for i in range(self.num_layers):\n",
    "            name = \"cell{}\".format(i)\n",
    "            name_reverse = \"cell_reverse{}\".format(i)\n",
    "            if i == 0:\n",
    "                for step in range(self.step):\n",
    "                    if step == 0:\n",
    "                        x_reverse = input[:, -(step + 1) * b:]\n",
    "                    else:\n",
    "                        x_reverse = input[:, -(step + 1) * b: -step * b]\n",
    "                    x_reverse = x_reverse.unsqueeze(dim=1)  # (1, 1, 12, 624, 352), (1, 1, 16, 312, 176)...\n",
    "            \n",
    "                    x = input[:, step * b:(step + 1) * b, :, :]  # (1, 12, 624, 352), (1, 16, 312, 176)...\n",
    "                    x = x.unsqueeze(dim=1)  # (1, 1, 12, 624, 352), (1, 1, 16, 312, 176)...\n",
    "            \n",
    "                    bsize, _, dimension, height, width = x.size()\n",
    "                \n",
    "                    if step == 0:\n",
    "                        (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i], \n",
    "                                                                 shape=(dimension, height, width))\n",
    "                        internal_state.append((h, c))\n",
    "                    \n",
    "                        (h_reverse, c_reverse) = getattr(self, name_reverse).init_hidden(batch_size=bsize, \n",
    "                                                                                         hidden=self.hidden_channels[i], \n",
    "                                                                                         shape=(dimension, height, width))\n",
    "                        internal_state_reverse.append((h_reverse, c_reverse))\n",
    "                    \n",
    "                    # do forward\n",
    "                    (h, c) = internal_state[i]\n",
    "                    (h_reverse, c_reverse) = internal_state_reverse[i]\n",
    "                \n",
    "                    x, new_c = getattr(self, name)(x, h, c)\n",
    "                    internal_state[i] = (x, new_c)\n",
    "                \n",
    "                    x_reverse, new_c_reverse = getattr(self, name_reverse)(x_reverse, h_reverse, c_reverse)\n",
    "                    internal_state_reverse[i] = (x_reverse, new_c_reverse)\n",
    "                \n",
    "                    outputs.append(x)\n",
    "                    outputs_reverse.insert(0, x_reverse)\n",
    "                if self.num_layers == 1:\n",
    "                    result = outputs[-1] + outputs_reverse[-1]\n",
    "                    result = result[:, 0]\n",
    "                    for i in range(self.hidden_channels[i] - 1):\n",
    "                        result = torch.cat([result, x[:, i + 1]], dim=1)\n",
    "                    return result\n",
    "            else:\n",
    "                input = torch.cat([outputs[j] + outputs_reverse[j] for j in range(self.step)], dim=1)\n",
    "                b = self.hidden_channels[i - 1]\n",
    "                outputs = []\n",
    "                outputs_reverse = []\n",
    "                for step in range(self.step):\n",
    "                    if step == 0:\n",
    "                        x_reverse = input[:, -(step + 1) * b:]\n",
    "                    else:\n",
    "                        x_reverse = input[:, -(step + 1) * b: -step * b]\n",
    "            \n",
    "                    x = input[:, step * b:(step + 1) * b]  # (1, 8, 12, 624, 352), (1, 8, 16, 312, 176)...\n",
    "                \n",
    "                    bsize, _, dimension, height, width = x.size()\n",
    "                \n",
    "                    if step == 0:\n",
    "                        (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i], \n",
    "                                                                 shape=(dimension, height, width))\n",
    "                        internal_state.append((h, c))\n",
    "                    \n",
    "                        (h_reverse, c_reverse) = getattr(self, name_reverse).init_hidden(batch_size=bsize, \n",
    "                                                                                         hidden=self.hidden_channels[i], \n",
    "                                                                                         shape=(dimension, height, width))\n",
    "                        internal_state_reverse.append((h_reverse, c_reverse))\n",
    "                    \n",
    "                    # do forward\n",
    "                    (h, c) = internal_state[i]\n",
    "                    (h_reverse, c_reverse) = internal_state_reverse[i]\n",
    "                \n",
    "                    x, new_c = getattr(self, name)(x, h, c)\n",
    "                    internal_state[i] = (x, new_c)\n",
    "                \n",
    "                    x_reverse, new_c_reverse = getattr(self, name_reverse)(x_reverse, h_reverse, c_reverse)\n",
    "                    internal_state_reverse[i] = (x_reverse, new_c_reverse)\n",
    "                \n",
    "                    outputs.append(x)\n",
    "                    outputs_reverse.insert(0, x_reverse)\n",
    "                if i == self.num_layers - 1:\n",
    "                    result = outputs[-1] + outputs_reverse[-1]\n",
    "                    result = result[:, 0]\n",
    "                    for i in range(self.hidden_channels[i] - 1):\n",
    "                        result = torch.cat([result, x[:, i + 1]], dim=1)\n",
    "                    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):  # 需要指定输入通道数\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # (batch_size, num_channels, 1, 1)\n",
    "        \n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)  # (batch_size, num_channels, 1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = self.sigmoid(avg_out + max_out)\n",
    "        \n",
    "        y = x * out.view(out.size(0), out.size(1), 1, 1)\n",
    "        \n",
    "        y = y + residual\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        \n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch_size, num_channels, h, w)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # (batch_size, 1, h, w)\n",
    "        max_out = torch.max(x, dim=1, keepdim=True)\n",
    "        \n",
    "        out = torch.cat([avg_out, max_out], dim=1)  # (batch_size, 2, h, w)\n",
    "        out1 = self.conv1(out)\n",
    "        out2 = self.relu1(out1)\n",
    "        out = self.sigmoid(out2)  # (batch_size, 1, h, w)\n",
    "        \n",
    "        y = x * out.view(out.size(0), 1, out.size(-2), out.size(-1))\n",
    "        y = y + residual\n",
    "        return  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, planes):  # 需要指定输入通道数\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.ca = ChannelAttention(planes)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([self.ca(x), self.sa(x)], dim=1)  # (batch_size, planes * 2, h, w)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_block(block_channel, r, n):\n",
    "    cl_channel = int(block_channel / 8)\n",
    "    cl2_channel = int(cl_channel / 2)\n",
    "    gn_a = int(block_channel / 2)\n",
    "    # 注意分析每一个组成部分的输出形状，其中ConvLSTM将输入在光谱维度平均分为8份作为每一个时间步长的输入，最终输出维度为原始维度的1/2\n",
    "    # 而SS注意力机制正好恢复出原始维度\n",
    "    layers = (nn.Sequential(ConvLSTM(input_channels=1, hidden_channels=[4, 4], \n",
    "                                     kernel_size=(3, 5, 5), step=8, effective_step=7).cuda(), \n",
    "                            BasicBlock(gn_a), \n",
    "                            gn_relu(block_channel, r), ))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_channels': 103,\n",
       " 'num_classes': 9,\n",
       " 'block_channels': (96, 128, 192, 256),\n",
       " 'inner_dim': 128,\n",
       " 'reduction_ratio': 1.0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = cfg['model']['params']\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDGL(CVModule):\n",
    "    def __init__(self, config):\n",
    "        super(SSDGL, self).__init__(config)\n",
    "        r = int(4 * self.config.reduction_ratio)  # The group number of group normalization: 4\n",
    "        block1_channels = int(self.config.block_channels[0] * self.config.reduction_ratio / r) * r\n",
    "        block2_channels = int(self.config.block_channels[1] * self.config.reduction_ratio / r) * r\n",
    "        block3_channels = int(self.config.block_channels[2] * self.config.reduction_ratio / r) * r\n",
    "        block4_channels = int(self.config.block_channels[3] * self.config.reduction_ratio / r) * r\n",
    "        \n",
    "        self.feature_ops = nn.ModuleList([\n",
    "            conv3x3_gn_relu(self.config.in_channels, block1_channels, r),  # (batch_size, 96, 624, 352)\n",
    "            \n",
    "            repeat_block(block1_channels, r, self.config.num_blocks[0]),   # num_blocks=(1, 1, 1, 1)\n",
    "            nn.Identity(),\n",
    "            downsample2x(block1_channels, block2_channels),  # (batch_size, 128, 312, 176)\n",
    "            \n",
    "            repeat_block(block2_channels, r, self.config.num_blocks[1]),\n",
    "            nn.Identity(),\n",
    "            downsample2x(block2_channels, block3_channels),  # (batch_size, 192, 156, 88)\n",
    "            \n",
    "            repeat_block(block3_channels, r, self.config.num_blocks[2]),\n",
    "            nn.Identity(),\n",
    "            downsample2x(block3_channels, block4_channels),  # (batch_size, 256, 78, 44)\n",
    "            \n",
    "            repeat_block(block4_channels, r, self.config.num_blocks[3]),\n",
    "            nn.Identity(), \n",
    "        ])\n",
    "        inner_dim = int(self.config.inner_dim * self.config.reduction_ratio)\n",
    "        \n",
    "        self.BasicBlock_list = nn.ModuleList([\n",
    "            BasicBlock(inner_dim), \n",
    "            BasicBlock(inner_dim),\n",
    "            BasicBlock(inner_dim),\n",
    "            BasicBlock(inner_dim),\n",
    "        ])\n",
    "        self.spation_list = nn.ModuleList([\n",
    "            SpatialAttention(),\n",
    "            SpatialAttention(),\n",
    "            SpatialAttention(),\n",
    "            SpatialAttention(),\n",
    "        ])\n",
    "        self.reduce_1x1convs = nn.ModuleList([\n",
    "            nn.Conv2d(block1_channels, inner_dim, 1), \n",
    "            nn.Conv2d(block2_channels, inner_dim, 1), \n",
    "            nn.Conv2d(block3_channels, inner_dim, 1), \n",
    "            nn.Conv2d(block4_channels, inner_dim, 1),\n",
    "        ])\n",
    "        self.fuse_3x3convs = nn.ModuleList([\n",
    "            conv3x3_gn_relu(inner_dim, inner_dim, r), \n",
    "            conv3x3_gn_relu(inner_dim, inner_dim, r), \n",
    "            conv3x3_gn_relu(inner_dim, inner_dim, r), \n",
    "            nn.Conv2d(inner_dim, self.config.in_channels, 3, 1, 1), \n",
    "        ])\n",
    "        \n",
    "        self.cls_pred_conv = nn.Conv2d(self.config.in_channels, self.config.num_classes, 1)\n",
    "        \n",
    "    def top_down(self, top, lateral):\n",
    "        top2x = F.interpolate(top, scale_factor=2.0, mode='bilinear')\n",
    "        return top2x + lateral\n",
    "    \n",
    "    def forward(self, x, y=None, train_inds=None, **kwargs):\n",
    "        feat_list = []\n",
    "        for op in self.feature_ops:\n",
    "            x = op(x)\n",
    "            \n",
    "            if isinstance(op, nn.Identity):\n",
    "                feat_list.append(x)\n",
    "        inner_feat_list = [self.reduce_1x1convs[i](feat) for i, feat in enumerate(feat_list)]\n",
    "        \n",
    "        inner_feat_list.reverse()  # [(batch_size, 128, 78, 44), (batch_size, 128, 156, 88), ...]\n",
    "        out_feat_list = [self.fuse_3x3convs[0](inner_feat_list[0])]  # (batch_size, 128, 78, 44)\n",
    "        for i in range(len(inner_feat_list) - 1):\n",
    "            inner = slef.top_down(out_feat_list[i], inner_feat_list[i + 1])\n",
    "            out = self.fuse_3x3convs[i + 1](inner)\n",
    "            out_feat_list.append(out)\n",
    "        final_feat = out_feat_list[-1]  # (batch_size, 103, 624, 352) This is the final feature space!!!\n",
    "        \n",
    "        logit = self.cls_pred_conv(final_feat)  # (batch_size, 9, 624, 352)\n",
    "        if self.training:\n",
    "            loss_dict = {'cls_loss': self.loss(logit, y, train_inds, final_feat)}\n",
    "            return loss_dict\n",
    "        \n",
    "        return torch.softmax(logit, dim=1)  # (batch_size, 9, 624, 352)\n",
    "    \n",
    "    def loss(self, x, y, train_inds, final_feat):\n",
    "        beta = 0.9999\n",
    "        if dataset_path ==\"SSDGL.SSDGL_1_0_pavia\":\n",
    "            cls_num_list = [6631, 18649, 2099, 3064, 1345, 5029, 1330, 3682, 947]\n",
    "        elif dataset_path ==\"SSDGL.SSDGL_1_0_Indianpine\":\n",
    "            cls_num_list = [46, 1428, 830, 237, 483, 730, 28, 478, 20, 972, 2455, 593, 205, 1265, 386, 93]\n",
    "        elif dataset_path == \"SSDGL.SSDGL_1_0_salinas\":\n",
    "            cls_num_list = [2009, 3726, 1976, 1394, 2678, 3959, 3579, 11271, 6203, 3278, 1068,1927, 916, 1070, 7268, 1807]\n",
    "        elif dataset_path == \"SSDGL.SSDGL_1_0_HOS\":\n",
    "            cls_num_list = [1251, 1254, 697, 1244, 1242, 325,1268, 1244, 1252, 1227, 1235, 1233, 469, 428, 660]\n",
    "        else:\n",
    "            print(\"no cls_num_list\")\n",
    "        effective_num = 1.0 - np.power(beta, cls_num_list)\n",
    "        per_cls_weights = (1.0 - beta) / np.array(effective_num)\n",
    "        per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(cls_num_list)\n",
    "        # 有标签的样本中，数量越少的类别，它的损失权重越大\n",
    "        per_cls_weights = torch.FloatTensor(per_cls_weights).cuda()\n",
    "        \n",
    "        # (1, 624, 352) 注意!!!这里的程序十分关键，要好好分析为什么这样计算损失?\n",
    "        weighted_losses = F.cross_entropy(x, y.long() - 1, ignore_index=-1, reduction='none', \n",
    "                                          weight=per_cls_weights)\n",
    "        #print(weighted_losses.size())\n",
    "        weighted_cross_entropy_losses = weighted_losses.mul_(train_inds).sum() / train_inds.sum()\n",
    "        #print(weighted_cross_entropy_losses, train_inds.sum())\n",
    "        \n",
    "        losses = weighted_cross_entropy_losses + self.statistical_loss(y, train_inds, final_feat)\n",
    "        return losses\n",
    "    \n",
    "    def statistical_loss(self, y, train_inds, final_feat):  \n",
    "        # y: (1, 624, 352), train_inds: (1, 624, 352), final_feat: (1, 103, 624, 352)\n",
    "        lamb = 0.01\n",
    "        delta = 0.\n",
    "        y = y.squeeze()  # ground truth: (624, 352)\n",
    "        train_inds = train_inds.squeeze()  # (624, 352)\n",
    "        final_feat = final_feat.squeeze()  # (103, 624, 352)\n",
    "        \n",
    "        cls_list = torch.unique(y)  # (0, 1, 2, .., 9)\n",
    "        num_cls = len(cls_list) - 1  # 9\n",
    "        num_train = int(train_inds.sum())  # 90\n",
    "        feat_dimension = final_feat.size()[0]  # 103\n",
    "        \n",
    "        location = torch.where(train_inds == 1.)\n",
    "        label = y[location]\n",
    "        \n",
    "        feat_dict_per_class = dict()\n",
    "        for i in range(1, num_cls+1):\n",
    "            feat_inds = torch.where(label==i)\n",
    "            feat_dict_per_class[i] = final_feat[:, location[0][feat_inds], location[1][feat_inds]]\n",
    "            \n",
    "        ck = dict()\n",
    "        for i in range(1, num_cls+1):\n",
    "            ck[i] = feat_dict_per_class[i].mean(dim=1).unsqueeze(dim=1)\n",
    "            \n",
    "        variance_loss = torch.tensor(0.).cuda()\n",
    "        for i in range(1, num_cls+1):\n",
    "            zj_ck = feat_dict_per_class[i] - ck[i]  # (103, num_train)\n",
    "            variance_loss += zj_ck.mul(zj_ck).sum() / (zj_ck.size()[1] - 1)\n",
    "        variance_loss = variance_loss / num_cls\n",
    "        \n",
    "        diver_loss = torch.tensor(0.).cuda()\n",
    "        for k in range(1, num_cls+1):\n",
    "            Sk = torch.zeros(feat_dimension, feat_dimension).cuda()\n",
    "            zj_ck = feat_dict_per_class[k] - ck[k]\n",
    "            nk = zj_ck.size()[1]\n",
    "            for i in range(nk):\n",
    "                Sk += torch.mm(zj_ck[:, i].unsqueeze(dim=1), zj_ck[:, i].unsqueeze(dim=0))\n",
    "            for t in range(k+1, num_cls+1):\n",
    "                St = torch.zeros(feat_dimension, feat_dimension).cuda()\n",
    "                zj_ct = feat_dict_per_class[t] - ck[t]\n",
    "                nt = zj_ct.size()[1]\n",
    "                for j in range(nt):\n",
    "                    St += torch.mm(zj_ct[:, j].unsqueeze(dim=1), zj_ct[:, j].unsqueeze(dim=0))\n",
    "                ck_ct = ck[k] - ck[t]\n",
    "                diver_loss += delta - torch.mm(torch.mm(ck_ct.transpose(1, 0), torch.inverse(Sk + St)), \n",
    "                                               ck_ct).squeeze() * (nk*nt - 2*nk*nt/(nk+nt))\n",
    "        diver_loss = diver_loss * lamb\n",
    "                \n",
    "        return variance_loss + diver_loss\n",
    "    \n",
    "    def set_defalut_config(self):\n",
    "        # pavia\n",
    "        self.config.update(dict(\n",
    "            in_channels=103, \n",
    "            num_classes=9, \n",
    "            block_channels=(96, 128, 192, 256), \n",
    "            num_blocks=(1, 1, 1, 1), \n",
    "            inner_dim=128, \n",
    "            reduction_ratio=1.0, \n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 分步测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds_list = []\n",
    "for idx, (im, mask, w) in enumerate(traindata_loader):\n",
    "    train_inds_list.append(w)\n",
    "    \n",
    "train_inds = train_inds_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mask.squeeze()\n",
    "train_inds = train_inds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feat = torch.randn(103, 624, 352).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_list = torch.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cls = len(cls_list) - 1\n",
    "num_train = int(train_inds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = torch.where(train_inds == 1.)\n",
    "label = y[location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 28,  50,  68, 102, 105, 107, 146, 149, 151, 154, 162, 164, 168, 173,\n",
       "         183, 185, 194, 194, 200, 202, 202, 204, 213, 216, 223, 227, 232, 240,\n",
       "         247, 254, 255, 274, 280, 295, 304, 307, 307, 307, 315, 315, 318, 325,\n",
       "         325, 328, 334, 335, 337, 337, 338, 338, 345, 347, 350, 351, 357, 358,\n",
       "         360, 366, 366, 372, 373, 395, 406, 406, 429, 433, 438, 442, 442, 451,\n",
       "         454, 463, 468, 492, 518, 523, 529, 534, 535, 542, 551, 553, 564, 565,\n",
       "         566, 569, 588, 598, 601, 604]),\n",
       " tensor([ 78, 122, 149, 155,  66,  28, 191, 128, 177,  50,  51, 115, 134, 132,\n",
       "         121, 149, 138, 197,  14, 129, 140, 155,  38,  75, 148, 152,  21, 177,\n",
       "          39, 163,  34, 204, 202, 190, 205,   2,  15, 187, 121, 208, 200, 110,\n",
       "         132,  14,  79, 134, 123, 230,  17,  80, 169, 142, 164, 139, 144, 154,\n",
       "         154,  31, 143,  38, 156,  72,  61, 295,  74,  90, 139,  10, 142, 191,\n",
       "         184,  86,  14,  62,  88, 203, 164, 145, 141,   9, 238, 156, 172,  68,\n",
       "          67,  56, 196, 260,  41,   1]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 4., 1., 2., 2., 4., 5., 1., 8., 8., 9., 5., 5., 9., 5., 5., 1.,\n",
       "        1., 9., 5., 5., 8., 8., 5., 5., 4., 5., 1., 9., 4., 6., 6., 6., 6., 3.,\n",
       "        3., 6., 7., 6., 6., 7., 7., 3., 4., 9., 7., 6., 3., 4., 6., 7., 6., 7.,\n",
       "        7., 7., 7., 3., 7., 3., 9., 3., 3., 1., 3., 9., 8., 9., 8., 9., 9., 8.,\n",
       "        3., 2., 8., 2., 1., 4., 4., 1., 2., 4., 2., 8., 8., 4., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict_per_class = dict()\n",
    "train_per_class = []\n",
    "for i in range(1, num_cls+1):\n",
    "    feat_inds = torch.where(label==i)[0]\n",
    "    train_per_class.append(len(feat_inds))\n",
    "    feat_dict_per_class[i] = final_feat[:, location[0][feat_inds], location[1][feat_inds]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = feat_dict_per_class[1]\n",
    "for i in range(1, num_cls):\n",
    "    feat = torch.cat([feat, feat_dict_per_class[i+1]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103, 90])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = torch.zeros((num_train, num_train), device=torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_train):\n",
    "    anchor = torch.unsqueeze(feat[:, i], 1)\n",
    "    distance[i, :] = torch.norm(anchor - feat, p=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = torch.unsqueeze(feat[:, 0], 1)\n",
    "c = torch.norm(anchor - feat, p=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, 13.8653, 14.2857, 13.6392, 14.5955, 15.6235, 15.7290, 14.3485,\n",
       "        14.4833, 15.2859, 14.5438, 13.6323, 13.3772, 15.5281, 15.3592, 13.0736,\n",
       "        14.3455, 14.9672, 12.5538, 12.8872, 15.4634, 13.8352, 13.9301, 13.2862,\n",
       "        14.8436, 13.3855, 13.9725, 14.1641, 14.0739, 13.2864, 15.9125, 13.7636,\n",
       "        13.3306, 14.1956, 13.8764, 14.0034, 12.8655, 14.0232, 14.8358, 13.2024,\n",
       "        13.5184, 14.5184, 13.2835, 15.0811, 14.1817, 13.9105, 13.1548, 13.8443,\n",
       "        14.3088, 15.2761, 14.2746, 15.4775, 14.1392, 13.7332, 14.5262, 14.4344,\n",
       "        13.4065, 14.0440, 14.1902, 13.2463, 15.0064, 13.9796, 14.7549, 14.2155,\n",
       "        13.2031, 14.0104, 13.1387, 14.3166, 11.7791, 12.5361, 15.1210, 16.0219,\n",
       "        13.4867, 13.9428, 13.9933, 14.0897, 14.0433, 12.7283, 15.2156, 14.1142,\n",
       "        13.7853, 13.9876, 12.7503, 15.1429, 13.8540, 15.8069, 14.2308, 13.9807,\n",
       "        13.1221, 13.6015], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = feat[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.6015, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(anchor, feat[:, 89], p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.7261, device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance[45, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.7261, device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance[50, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_per_class = [5, 10, 10, 10, 10, 10, 5, 10, 5, 10, 10, 10, 10, 10, 10, 5]\n",
    "num_cls = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_index = np.zeros(num_cls).astype(np.int16)\n",
    "for i in range(num_cls):\n",
    "    acc_index[i] = sum(train_per_class[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,  15,  25,  35,  45,  55,  60,  70,  75,  85,  95, 105, 115,\n",
       "       125, 135, 140], dtype=int16)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "\n",
    "bisect_fn = bisect.bisect\n",
    "position = bisect_fn(acc_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if position == 0:\n",
    "    left_index = 0\n",
    "else:\n",
    "    left_index = acc_index[position-1]\n",
    "right_index = acc_index[position]\n",
    "left_index, right_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), torch.Size([75]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance[0, :left_index].size(), distance[0, right_index:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "result = torch.zeros(num_train).cuda()\n",
    "for i in range(90):\n",
    "    position = bisect_fn(acc_index, i)\n",
    "    if position == 0:\n",
    "        left_index = 0\n",
    "    else:\n",
    "        left_index = acc_index[position-1]\n",
    "    right_index = acc_index[position]\n",
    "    #print(left_index, right_index)\n",
    "    intra_dist = distance[i, left_index:right_index]\n",
    "    inter_dist = torch.cat([distance[i, :left_index], distance[i, right_index:]])\n",
    "    #print(intra_dist.size(), inter_dist.size())\n",
    "    #print(torch.topk(intra_dist, k=4)[0], torch.topk(inter_dist, k=4, largest=False)[0])\n",
    "    positive = (torch.topk(intra_dist, k=4)[0]).sum() / 4\n",
    "    negative = (torch.topk(inter_dist, k=4, largest=False)[0]).sum() / 4\n",
    "    result[i] = alpha + positive - negative\n",
    "    if result[i] < 2.0:\n",
    "        result[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6971, 2.5929, 2.0472, 0.0000, 3.5177, 3.5436, 3.9423, 3.9382, 3.1814,\n",
       "        3.3953, 3.6421, 3.6516, 3.7077, 3.9328, 3.1526, 3.2969, 3.6764, 2.7728,\n",
       "        3.4938, 2.6953, 4.1464, 3.3536, 3.4477, 4.1294, 3.4954, 3.3877, 3.5933,\n",
       "        3.4263, 3.2547, 3.3897, 3.0738, 3.8040, 3.0195, 3.1208, 3.2751, 2.9413,\n",
       "        3.7101, 3.6262, 2.9235, 3.3959, 3.6894, 3.7830, 3.4623, 3.4566, 2.9977,\n",
       "        3.0198, 3.0355, 3.2235, 3.6272, 3.5818, 3.2053, 3.4514, 3.3535, 3.1393,\n",
       "        2.5718, 2.0719, 3.0380, 2.5163, 2.8320, 2.1598, 3.0866, 3.3315, 3.3731,\n",
       "        2.9848, 3.4181, 2.9928, 3.1267, 3.0138, 3.6353, 3.3168, 2.9110, 3.1164,\n",
       "        3.5157, 2.5362, 3.3207, 3.3088, 3.2438, 3.1809, 3.1998, 3.4505, 3.8424,\n",
       "        3.3883, 3.7903, 2.9894, 3.0427, 2.0168, 2.9740, 3.4137, 2.5979, 2.4448],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6971, 2.5929, 2.0472, 0.0000, 3.5177, 3.5436, 3.9423, 3.9382, 3.1814,\n",
       "        3.3953, 3.6421, 3.6516, 3.7077, 3.9328, 3.1526, 3.2969, 3.6764, 2.7728,\n",
       "        3.4938, 2.6953, 4.1464, 3.3536, 3.4477, 4.1294, 3.4954, 3.3877, 3.5933,\n",
       "        3.4263, 3.2547, 3.3897, 3.0738, 3.8040, 3.0195, 3.1208, 3.2751, 2.9413,\n",
       "        3.7101, 3.6262, 2.9235, 3.3959, 3.6894, 3.7830, 3.4623, 3.4566, 2.9977,\n",
       "        3.0198, 3.0355, 3.2235, 3.6272, 3.5818, 3.2053, 3.4514, 3.3535, 3.1393,\n",
       "        2.5718, 2.0719, 3.0380, 2.5163, 2.8320, 2.1598, 3.0866, 3.3315, 3.3731,\n",
       "        2.9848, 3.4181, 2.9928, 3.1267, 3.0138, 3.6353, 3.3168, 2.9110, 3.1164,\n",
       "        3.5157, 2.5362, 3.3207, 3.3088, 3.2438, 3.1809, 3.1998, 3.4505, 3.8424,\n",
       "        3.3883, 3.7903, 2.9894, 3.0427, 2.0168, 2.9740, 3.4137, 2.5979, 2.4448],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(288.1718, device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "train_inds_list = []\n",
    "for idx, (im, mask, w) in enumerate(traindata_loader):\n",
    "    train_inds_list.append(w)\n",
    "    \n",
    "train_inds = train_inds_list[0]\n",
    "\n",
    "y = mask.squeeze()\n",
    "train_inds = train_inds.squeeze()\n",
    "\n",
    "final_feat = torch.randn(103, 624, 352).cuda()\n",
    "\n",
    "cls_list = torch.unique(y)\n",
    "\n",
    "print(cls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cls = len(cls_list) - 1\n",
    "num_train = int(train_inds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 90)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cls, num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = torch.where(train_inds == 1.)\n",
    "label = y[location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict_per_class = dict()\n",
    "train_per_class = []\n",
    "for i in range(1, num_cls+1):\n",
    "    feat_inds = torch.where(label==i)[0]\n",
    "    train_per_class.append(len(feat_inds))\n",
    "    feat_dict_per_class[i] = final_feat[:, location[0][feat_inds], location[1][feat_inds]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = feat_dict_per_class[1]\n",
    "for i in range(1, num_cls):\n",
    "    feat = torch.cat([feat, feat_dict_per_class[i+1]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = torch.zeros((num_train, num_train), device=torch.device(\"cuda:0\"))\n",
    "\n",
    "for i in range(num_train):\n",
    "    anchor = torch.unsqueeze(feat[:, i], 1)\n",
    "    distance[i, :] = torch.norm(anchor - feat, p=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck = dict()\n",
    "for i in range(1, num_cls+1):\n",
    "    ck[i] = feat_dict_per_class[i].mean(dim=1).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = ck[1]\n",
    "for i in range(1, num_cls):\n",
    "    centers = torch.cat([centers, ck[i+1]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2014, device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radius = torch.norm(centers, p=2, dim=0).mean()\n",
    "radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1706,  0.0887, -0.5220, -0.0099, -0.2277, -0.1112, -0.3236,  0.0628,\n",
       "        -0.1747,  0.1720,  0.4116, -0.0681,  0.2853, -0.1676,  0.0126,  0.4607,\n",
       "        -0.2436,  0.1084,  0.4405, -0.2513,  0.0321, -0.2306, -0.3630, -0.2676,\n",
       "         0.1296, -0.2641, -0.1007,  0.5824,  0.6994, -0.0352, -0.5557,  0.7687,\n",
       "         0.3566, -0.2765,  0.0725,  0.6493,  0.2286, -0.2276, -0.0267,  0.2053,\n",
       "         0.1496,  0.4600, -0.4200,  0.2003,  0.1398,  0.0827, -0.1850, -0.2661,\n",
       "         0.2280, -0.1412, -0.0511, -0.0246, -0.2369,  0.4583, -0.0242, -0.3626,\n",
       "         0.0987, -0.7115, -0.2526,  0.0406,  0.0855,  0.1035, -0.3285,  0.1473,\n",
       "         0.2018, -0.3990,  0.3077,  0.5156, -0.5354,  0.0314, -0.3164, -0.1999,\n",
       "        -0.1038, -0.5451, -0.1978, -0.1962,  0.5864,  0.3214,  0.2991, -0.1843,\n",
       "        -0.0469, -0.2046, -0.0845, -0.2531,  0.1264,  0.6228,  0.2213,  0.2646,\n",
       "        -0.2768, -0.0662,  0.1092,  0.5880, -0.2813, -0.0418,  0.3219,  0.1254,\n",
       "         0.1677, -0.2811, -0.1233,  0.1378,  0.0219,  0.3091, -0.0875],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7301, 3.0787, 3.3628, 2.9282, 2.8457, 3.4343, 3.4883, 2.9951, 2.9491],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(centers, p=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_trans = centers / torch.norm(centers, p=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0554,  0.0288, -0.1696, -0.0032, -0.0740, -0.0361, -0.1051,  0.0204,\n",
       "        -0.0567,  0.0559,  0.1337, -0.0221,  0.0927, -0.0544,  0.0041,  0.1496,\n",
       "        -0.0791,  0.0352,  0.1431, -0.0816,  0.0104, -0.0749, -0.1179, -0.0869,\n",
       "         0.0421, -0.0858, -0.0327,  0.1892,  0.2272, -0.0114, -0.1805,  0.2497,\n",
       "         0.1158, -0.0898,  0.0235,  0.2109,  0.0742, -0.0739, -0.0087,  0.0667,\n",
       "         0.0486,  0.1494, -0.1364,  0.0651,  0.0454,  0.0268, -0.0601, -0.0864,\n",
       "         0.0741, -0.0459, -0.0166, -0.0080, -0.0770,  0.1489, -0.0079, -0.1178,\n",
       "         0.0321, -0.2311, -0.0820,  0.0132,  0.0278,  0.0336, -0.1067,  0.0478,\n",
       "         0.0656, -0.1296,  0.0999,  0.1675, -0.1739,  0.0102, -0.1028, -0.0649,\n",
       "        -0.0337, -0.1771, -0.0643, -0.0637,  0.1905,  0.1044,  0.0972, -0.0599,\n",
       "        -0.0152, -0.0664, -0.0274, -0.0822,  0.0411,  0.2023,  0.0719,  0.0859,\n",
       "        -0.0899, -0.0215,  0.0355,  0.1910, -0.0914, -0.0136,  0.1045,  0.0407,\n",
       "         0.0545, -0.0913, -0.0400,  0.0448,  0.0071,  0.1004, -0.0284],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers_trans[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_trans = centers_trans * radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0554,  0.0288, -0.1696, -0.0032, -0.0740, -0.0361, -0.1051,  0.0204,\n",
       "        -0.0567,  0.0559,  0.1337, -0.0221,  0.0927, -0.0544,  0.0041,  0.1496,\n",
       "        -0.0791,  0.0352,  0.1431, -0.0816,  0.0104, -0.0749, -0.1179, -0.0869,\n",
       "         0.0421, -0.0858, -0.0327,  0.1892,  0.2272, -0.0114, -0.1805,  0.2497,\n",
       "         0.1158, -0.0898,  0.0235,  0.2109,  0.0742, -0.0739, -0.0087,  0.0667,\n",
       "         0.0486,  0.1494, -0.1364,  0.0651,  0.0454,  0.0268, -0.0601, -0.0864,\n",
       "         0.0741, -0.0459, -0.0166, -0.0080, -0.0770,  0.1489, -0.0079, -0.1178,\n",
       "         0.0321, -0.2311, -0.0820,  0.0132,  0.0278,  0.0336, -0.1067,  0.0478,\n",
       "         0.0656, -0.1296,  0.0999,  0.1675, -0.1739,  0.0102, -0.1028, -0.0649,\n",
       "        -0.0337, -0.1771, -0.0643, -0.0637,  0.1905,  0.1044,  0.0972, -0.0599,\n",
       "        -0.0152, -0.0664, -0.0274, -0.0822,  0.0411,  0.2023,  0.0719,  0.0859,\n",
       "        -0.0899, -0.0215,  0.0355,  0.1910, -0.0914, -0.0136,  0.1045,  0.0407,\n",
       "         0.0545, -0.0913, -0.0400,  0.0448,  0.0071,  0.1004, -0.0284],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers_trans[:, 1] / radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_index = np.zeros(num_cls).astype(np.int16)\n",
    "for i in range(num_cls):\n",
    "    acc_index[i] = sum(train_per_class[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "result = torch.zeros(num_train).cuda()\n",
    "for i in range(90):\n",
    "    position = bisect_fn(acc_index, i)\n",
    "    if position == 0:\n",
    "        left_index = 0\n",
    "    else:\n",
    "        left_index = acc_index[position-1]\n",
    "    right_index = acc_index[position]\n",
    "    #print(left_index, right_index)\n",
    "    intra_dist = distance[i, left_index:right_index]\n",
    "    inter_dist = torch.cat([distance[i, :left_index], distance[i, right_index:]])\n",
    "    #print(intra_dist.size(), inter_dist.size())\n",
    "    #print(torch.topk(intra_dist, k=4)[0], torch.topk(inter_dist, k=4, largest=False)[0])\n",
    "    positive = (torch.topk(intra_dist, k=4)[0]).sum() / 4\n",
    "    negative = (torch.topk(inter_dist, k=4, largest=False)[0]).sum() / 4\n",
    "    result[i] = alpha + positive - negative\n",
    "    if result[i] < 2.0:\n",
    "        result[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ind = bisect_fn(acc_index, 89)\n",
    "class_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(torch.norm((feat[:, 0].unsqueeze(dim=1)-centers), p=2, dim=0), dim=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers_trans[:, 0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(46.7359, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "result = torch.zeros(num_train).cuda()\n",
    "for i in range(num_train):\n",
    "    class_ind = bisect_fn(acc_index, i)\n",
    "    #print(class_ind)\n",
    "    if class_ind == 0:\n",
    "        negative_centers = centers[:, 1:]\n",
    "    else:\n",
    "        negative_centers = torch.cat([centers[:, :class_ind], centers[:, class_ind+1:]], dim=1)\n",
    "    #print(negative_centers.size())\n",
    "    closest_negative_center_index = torch.min(torch.norm(feat[:, i].unsqueeze(dim=1) - negative_centers, \n",
    "                                                         p=2, dim=0), dim=0)[1]\n",
    "    closest_negative_center = negative_centers[:, closest_negative_center_index]\n",
    "    closest_negative_center_trans = closest_negative_center / torch.norm(closest_negative_center, p=2) * radius\n",
    "    interclass_dists = torch.square(torch.dist(centers_trans[:, class_ind], closest_negative_center_trans, p=2))\n",
    "    #print(centers_dists)\n",
    "    intraclass_dists = torch.square(torch.dist(feat[:, i], centers_trans[:, class_ind], p=2))\n",
    "    result[i] =  intraclass_dists - interclass_dists * 0.2 + radius **2 / 2\n",
    "    if result[i] < 0.0:\n",
    "        result[i] = 0.0\n",
    "    #print(result[i])\n",
    "print(result.sum() / (2 * num_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_negative_center_index = torch.min(torch.norm(feat[:, 0].unsqueeze(dim=1) - centers, \n",
    "                                                         p=2, dim=0), dim=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_negative_center_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.square(torch.dist(centers_trans[:, 0], centers_trans[:, closest_negative_center_index], p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_loss = torch.tensor(0.).cuda()\n",
    "for i in range(1, num_cls+1):\n",
    "    zj_ck = feat_dict_per_class[i] - ck[i]  # (103, num_train)\n",
    "    variance_loss += zj_ck.mul(zj_ck).sum() / (zj_ck.size()[1] - 1)\n",
    "variance_loss = variance_loss / num_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(102.3473, device='cuda:0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dimension = final_feat.size()[0]  # 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 1e-10\n",
    "delta = 0.\n",
    "diver_loss = torch.tensor(0.).cuda()\n",
    "for k in range(1, num_cls+1):\n",
    "    Sk = torch.zeros(feat_dimension, feat_dimension).cuda()\n",
    "    zj_ck = feat_dict_per_class[k] - ck[k]\n",
    "    nk = zj_ck.size()[1]\n",
    "    for i in range(nk):\n",
    "        Sk += torch.mm(zj_ck[:, i].unsqueeze(dim=1), zj_ck[:, i].unsqueeze(dim=0))\n",
    "    for t in range(k+1, num_cls+1):\n",
    "        #print(k, t)\n",
    "        St = torch.zeros(feat_dimension, feat_dimension).cuda()\n",
    "        zj_ct = feat_dict_per_class[t] - ck[t]\n",
    "        nt = zj_ct.size()[1]\n",
    "        for j in range(nt):\n",
    "            St += torch.mm(zj_ct[:, j].unsqueeze(dim=1), zj_ct[:, j].unsqueeze(dim=0))\n",
    "        \n",
    "        ck_ct = ck[k] - ck[t]\n",
    "        diver_loss += delta - torch.mm(torch.mm(ck_ct.transpose(1, 0), torch.inverse(Sk + St)), \n",
    "                                       ck_ct).squeeze() * (nk*nt - 2*nk*nt/(nk+nt))\n",
    "diver_loss = diver_loss * lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5661, device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diver_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1621, device='cuda:0')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diver_loss = torch.tensor(0.).cuda()\n",
    "for k in range(1, num_cls+1):\n",
    "    for t in range(k+1, num_cls+1):\n",
    "        diver_loss -= torch.dist(ck[k], ck[t], p=2)\n",
    "diver_loss * 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1318, device='cuda:0')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diver_loss = torch.tensor(0.).cuda()\n",
    "for k in range(1, num_cls+1):\n",
    "    for t in range(k+1, num_cls+1):\n",
    "        diver_loss -= torch.dist(ck[k], ck[t], p=1)\n",
    "diver_loss * 1e-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
